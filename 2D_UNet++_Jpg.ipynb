{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet++: A Nested U-Net Architecture for Medical Image Segmentation (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7329239/)\n",
    "# This code follows the tructure from the paper \n",
    "# Properly working\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Dropout, Input, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy\n",
    "from contextlib import redirect_stdout\n",
    "import time\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "#from keras.models import load_model\n",
    "\n",
    "\n",
    "#Start Execution Time\n",
    "start_time = time.time()\n",
    "\n",
    "path_out = \"...\"\n",
    "path_in_tumor = \"...\"\n",
    "path_in_mask = \"...\"\n",
    "\n",
    "\n",
    "if not os.path.exists(path_out):\n",
    "    os.makedirs(path_out)\n",
    "    print(\"Directory Created \")\n",
    "else:    \n",
    "    print(\"Directory already exists\") \n",
    "\n",
    "# Downsamples (Resizes) image to target size\n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True,)\n",
    "\n",
    "# Upsamples (Resizes) image to target size\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "\n",
    "# Function for a Convolution layer containing 2 blocks\n",
    "def conv2d_block( input_tensor, n_filters, kernel_size = (3,3), name=\"contraction\"):\n",
    "  \"Add 2 conv layer\"\n",
    "  x = Conv2D(filters=n_filters, kernel_size=kernel_size, kernel_initializer='he_normal', \n",
    "             padding='same',activation=\"relu\", name=name+'_1')(input_tensor)\n",
    "  \n",
    "  x = Conv2D(filters=n_filters, kernel_size=kernel_size, kernel_initializer='he_normal', \n",
    "             padding='same',activation=\"relu\",name=name+'_2')(x)\n",
    "  return x\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    smooth = 1\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "def bce_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "def loss_func(y_true, y_pred):\n",
    "    d_loss = dice_coef_loss(y_true, y_pred)\n",
    "    b_loss = bce_loss(y_true, y_pred)\n",
    "    return (0.5*d_loss)+(0.5*b_loss)\n",
    "\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "files_images = glob.glob (path_in_tumor + \"*.jpg\") \n",
    "for myFile in files_images:\n",
    "    im = cv2.imread (myFile, cv2.IMREAD_GRAYSCALE)\n",
    "    im = cv2.resize(im, (512, 512))\n",
    "    #im = im/127.5 - 1\n",
    "    images.append (im)\n",
    "    \n",
    "files_masks = glob.glob (path_in_mask + \"*.jpg\") \n",
    "for myFile in files_masks:\n",
    "    im1 = cv2.imread (myFile, cv2.IMREAD_GRAYSCALE)\n",
    "    im1 = cv2.resize(im1, (512, 512))\n",
    "    #im1 = im1/127.5 - 1\n",
    "    masks.append (im1)\n",
    "\n",
    "images = np.array(images, dtype= 'float32') \n",
    "images = np.clip((images/12728),0,1)\n",
    "masks = np.array(masks, dtype= 'float32') * 1\n",
    "\n",
    "print(images.shape)\n",
    "print(masks.shape)\n",
    "\n",
    "\n",
    "img_size_ori = 512\n",
    "img_size_target = 128\n",
    "\n",
    "\n",
    "# Expands image to 3D by adding 1 as channel\n",
    "images = np.expand_dims(images,axis=-1)\n",
    "masks = np.expand_dims(masks,axis=-1)\n",
    "\n",
    "# Reshaping to 128 x 128\n",
    "images = np.array([ downsample(image) for image in images ])\n",
    "masks = (np.array([ downsample(mask) for mask in masks ])>0)*1\n",
    "print(images.shape)\n",
    "print(masks.shape)\n",
    "\n",
    "\n",
    "# 18 random input images with tumor highlighted\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i, idx in enumerate(np.random.randint(images.shape[0], size=18), start=1):\n",
    "    plt.subplot(3, 6, i)\n",
    "    plt.imshow(images[idx], cmap='gray')\n",
    "    plt.imshow(np.ones_like(masks[idx])-masks[idx], alpha=0.5, cmap='Set1')\n",
    "    plt.title('MRI')\n",
    "    plt.axis('off')\n",
    "plt.savefig(path_out + 'Sample_Image_Plot.jpg')\n",
    "\n",
    "# 9 random input tumor images and their corresponding masks\n",
    "plt.figure(figsize=(12, 5))\n",
    "i=1\n",
    "for idx in np.random.randint( images.shape[0], size=9):\n",
    "  plt.subplot(3,6,i);i+=1\n",
    "  plt.imshow(images[idx])\n",
    "  plt.title(\"Train Image\")\n",
    "  plt.axis('off')\n",
    "  plt.subplot(3,6,i);i+=1\n",
    "  plt.imshow(masks[idx]) \n",
    "  plt.title(\"Train Mask\")\n",
    "  plt.axis('off')\n",
    "plt.savefig(path_out + 'Sample_Image_Mask_Plot.jpg')\n",
    "\n",
    "# Training Test division\n",
    "X_train,X_test,Y_train,Y_test = train_test_split( images,masks,test_size=0.2)\n",
    "del images\n",
    "del masks\n",
    "\n",
    "gc.collect()\n",
    "X_train.shape,X_test.shape\n",
    "tr_data = str(Y_train.shape)\n",
    "ts_data = str(Y_test.shape)\n",
    "\n",
    "# Training is divided into training and validation\n",
    "X_train1,X_val,Y_train1,Y_val = train_test_split(X_train,Y_train,test_size=0.2)\n",
    "\n",
    "del X_train\n",
    "del Y_train\n",
    "\n",
    "gc.collect()\n",
    "X_train1.shape,X_val.shape\n",
    "\n",
    "file0 = open(path_out + 'Data_division.txt','w')\n",
    "file0.write(\"Initial Training Data: \" + str(tr_data)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Test Data: \" + str(ts_data)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Final Training Data: \" + str(Y_train1.shape)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Validation Data: \" + str(Y_val.shape)) \n",
    "file0.close()\n",
    "\n",
    "\n",
    "# Flips array in the left/right direction.\n",
    "X_train1 = np.append( X_train1, [ np.fliplr(x) for x in X_train1], axis=0 )\n",
    "Y_train1 = np.append( Y_train1, [ np.fliplr(y) for y in Y_train1], axis=0 )\n",
    "X_train1.shape,Y_train1.shape\n",
    "\n",
    "# Converting train and validation data with image data generator\n",
    "train_datagen = ImageDataGenerator(brightness_range=(0.9,1.1),\n",
    "                                   zoom_range=[.9,1.1],\n",
    "                                   fill_mode='nearest')\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow(X_train1, Y_train1, batch_size=8)\n",
    "val_generator = val_datagen.flow(X_val, Y_val, batch_size=8)\n",
    "\n",
    "\n",
    "# Initial input with initial assigned shape\n",
    "IMG_DIM = (128,128,1)\n",
    "\n",
    "inp = Input( shape=IMG_DIM )\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# X(0,0)\n",
    "d1 = conv2d_block( inp, 64, name=\"contraction_1\")\n",
    "p1 = MaxPooling2D( pool_size=(2,2), strides=(2,2))(d1)\n",
    "p1 = BatchNormalization(momentum=0.8)(p1)\n",
    "p1 = Dropout(0.1)(p1)\n",
    "\n",
    "# X(1,0)\n",
    "d2 = conv2d_block( p1, 128, name=\"contraction_2_1\" )\n",
    "p2 = MaxPooling2D(pool_size=(2,2), strides=(2,2) )(d2)\n",
    "p2 = BatchNormalization(momentum=0.8)(p2)\n",
    "p2 = Dropout(0.1)(p2)\n",
    "\n",
    "# X(2,0)\n",
    "d3 = conv2d_block( p2, 256, name=\"contraction_3_1\")\n",
    "p3 = MaxPooling2D(pool_size=(2,2), strides=(2,2) )(d3)\n",
    "p3 = BatchNormalization(momentum=0.8)(p3)\n",
    "p3 = Dropout(0.1)(p3)\n",
    "\n",
    "# X(3,0)\n",
    "d4 = conv2d_block(p3,512, name=\"contraction_4_1\")\n",
    "p4 = MaxPooling2D(pool_size=(2,2), strides=(2,2) )(d4)\n",
    "p4 = BatchNormalization(momentum=0.8)(p4)\n",
    "p4 = Dropout(0.1)(p4)\n",
    "\n",
    "# X(4,0)\n",
    "d5 = conv2d_block(p4,512, name=\"contraction_5_1\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ---------------- UNet++ L1 ---------------\n",
    "# X(0,1)\n",
    "du_01 = Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = 'same')(d2)\n",
    "du_01 = concatenate([du_01,d1])\n",
    "du_01 = Dropout(0.1)(du_01)\n",
    "c_01 = conv2d_block(du_01, 64, name=\"expansion_01\")\n",
    "\n",
    "# ---------------- UNet++ L2 ---------------\n",
    "# X(1,1)\n",
    "du_11 = Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = 'same')(d3)\n",
    "du_11 = concatenate([du_11,d2])\n",
    "du_11 = Dropout(0.1)(du_11)\n",
    "c_11 = conv2d_block(du_11, 128, name=\"expansion_11\")\n",
    "\n",
    "# X(0,2)\n",
    "du_02 = Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = 'same')(c_11)\n",
    "du_02 = concatenate([du_02,du_01,d1])\n",
    "du_02 = Dropout(0.1)(du_02)\n",
    "c_02 = conv2d_block(du_02, 64, name=\"expansion_02\")\n",
    "\n",
    "# ---------------- UNet++ L3 ---------------\n",
    "# X(2,1)\n",
    "du_21 = Conv2DTranspose(256, (3, 3), strides = (2, 2), padding = 'same')(d4)\n",
    "du_21 = concatenate([du_21,d3])\n",
    "du_21 = Dropout(0.1)(du_21)\n",
    "c_21 = conv2d_block(du_21, 256, name=\"expansion_21\")\n",
    "\n",
    "# X(1,2)\n",
    "du_12 = Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = 'same')(c_21)\n",
    "du_12 = concatenate([du_12,c_11,d2])\n",
    "du_12 = Dropout(0.1)(du_12)\n",
    "c_12 = conv2d_block(du_12, 128, name=\"expansion_12\")\n",
    "\n",
    "# X(0,3)\n",
    "du_03 = Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = 'same')(c_12)\n",
    "du_03 = concatenate([du_03,c_02,c_01,d1])\n",
    "du_03 = Dropout(0.1)(du_03)\n",
    "c_03 = conv2d_block(du_03, 64, name=\"expansion_03\")\n",
    "\n",
    "# ---------------- UNet++ L4 ---------------\n",
    "# X(3,1)\n",
    "u_31 = Conv2DTranspose(256, (3, 3), strides = (2, 2), padding = 'same')(d5)\n",
    "u_31 = concatenate([u_31,d4])\n",
    "u_31 = Dropout(0.1)(u_31)\n",
    "c_31 = conv2d_block(u_31, 256, name=\"expansion_1\")\n",
    "\n",
    "# X(2,2)\n",
    "u_22 = Conv2DTranspose(256, (3, 3), strides = (2, 2), padding = 'same')(c_31)\n",
    "u_22 = concatenate([u_22,c_21,d3])\n",
    "u_22 = Dropout(0.1)(u_22)\n",
    "c_22 = conv2d_block(u_22, 256, name=\"expansion_2\")\n",
    "\n",
    "# X(1,3)\n",
    "u_13 = Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = 'same')(c_22)\n",
    "u_13 = concatenate([u_13,c_12,c_11,d2])\n",
    "u_13 = Dropout(0.1)(u_13)\n",
    "c_13 = conv2d_block(u_13, 128, name=\"expansion_3\")\n",
    "\n",
    "# X(0,4)\n",
    "u_04 = Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = 'same')(c_13)\n",
    "u_04 = concatenate([u_04,c_03,c_02,c_01,d1])\n",
    "u_04 = Dropout(0.1)(u_04)\n",
    "c_04 = conv2d_block(u_04, 64, name=\"expansion_4\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Final concatenation\n",
    "c_L = concatenate([c_01,c_02,c_03,c_04])\n",
    "\n",
    "out = Conv2D(1, (1,1), name=\"output\", activation='sigmoid')(c_L)\n",
    "\n",
    "unetpp = Model( inp, out )\n",
    "unetpp.summary()\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "with open(path_out + 'modelsummary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        unetpp.summary()\n",
    "\n",
    "unetpp.compile(optimizer=optimizers.Adam(lr=1e-3), \n",
    "             loss=loss_func, metrics=['acc', dice_coef])\n",
    "\n",
    "\n",
    "model_checkpoint  = ModelCheckpoint(path_out + 'model_best_checkpoint.h5', save_best_only=True, \n",
    "                                    monitor='val_loss', mode='min', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "reduceLR = ReduceLROnPlateau(patience=4, verbose=2, monitor='val_loss',min_lr=1e-4, mode='min')\n",
    "\n",
    "callback_list = [early_stopping, reduceLR, model_checkpoint]\n",
    "\n",
    "hist = unetpp.fit(X_train1,Y_train1,batch_size=8,epochs=100,\n",
    "               validation_data=(X_val,Y_val),verbose=1)\n",
    "\n",
    "\n",
    "# Calculating and drawing training plots\n",
    "acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "dice = hist.history['dice_coef']\n",
    "val_dice = hist.history['val_dice_coef']\n",
    "\n",
    "avg_acc = np.mean(acc) * 100\n",
    "avg_val_acc = np.mean(val_acc) * 100\n",
    "avg_loss = np.mean(loss) * 100\n",
    "avg_val_loss = np.mean(val_loss) * 100\n",
    "avg_dcc = np.mean(dice) * 100\n",
    "avg_val_dcc = np.mean(val_dice) * 100\n",
    "\n",
    "file0 = open(path_out + 'avg_eval.txt','w')#append mode \n",
    "file0.write(\"Average Training Accuracy: \" + str(avg_acc)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Average Validation Accuracy: \" + str(avg_val_acc)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Average Training DCC: \" + str(avg_dcc)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Average Validation DCC: \" + str(avg_val_dcc)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Average Training Loss: \" + str(avg_loss)) \n",
    "file0.write(\"\\n\")\n",
    "file0.write(\"Average Validation Loss: \" + str(avg_val_loss)) \n",
    "file0.close()\n",
    "\n",
    "np.savetxt(path_out + 'train_acc.txt', acc, delimiter=\"\\n\") \n",
    "np.savetxt(path_out + 'val_acc.txt', val_acc, delimiter=\"\\n\") \n",
    "np.savetxt(path_out + 'train_loss.txt', loss, delimiter=\"\\n\") \n",
    "np.savetxt(path_out + 'val_loss.txt', val_loss, delimiter=\"\\n\") \n",
    "np.savetxt(path_out + 'dcc.txt', dice, delimiter=\"\\n\") \n",
    "np.savetxt(path_out + 'val_dcc.txt', val_dice, delimiter=\"\\n\") \n",
    "\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 4))\n",
    "t = f.suptitle('Unet++ Performance in Segmenting Tumors', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "epoch_list = hist.epoch\n",
    "\n",
    "ax1.plot(epoch_list, hist.history['acc'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, hist.history['val_acc'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(0, epoch_list[-1], 5))\n",
    "ax1.set_ylabel('Accuracy Value');ax1.set_xlabel('Epoch');ax1.set_title('Accuracy')\n",
    "ax1.legend(loc=\"best\");ax1.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "ax2.plot(epoch_list, hist.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, hist.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(0, epoch_list[-1], 5))\n",
    "ax2.set_ylabel('Loss Value');ax2.set_xlabel('Epoch');ax2.set_title('Loss')\n",
    "ax2.legend(loc=\"best\");ax2.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "ax3.plot(epoch_list, hist.history['dice_coef'], label='Train DCC')\n",
    "ax3.plot(epoch_list, hist.history['val_dice_coef'], label='Validation DCC')\n",
    "ax3.set_xticks(np.arange(0, epoch_list[-1], 5))\n",
    "ax3.set_ylabel('DCC Value');ax3.set_xlabel('Epoch');ax3.set_title('DCC')\n",
    "ax3.legend(loc=\"best\");ax3.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.savefig(path_out + 'Performance_Plot.jpg')\n",
    "\n",
    "# Prediction\n",
    "THRESHOLD = 0.2\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_datagen = test_datagen.flow(X_test, Y_test, batch_size=32)\n",
    "predicted_mask = (unetpp.predict(X_test)>THRESHOLD)*1\n",
    "np.save(path_out + 'predictions.npy', predicted_mask)\n",
    "print(predicted_mask.shape)\n",
    "\n",
    "test_eval = unetpp.evaluate(X_test, Y_test, verbose=0)\n",
    "test_loss = test_eval[0] * 100\n",
    "test_acc = test_eval[1] * 100\n",
    "test_dcc = test_eval[2] * 100\n",
    "print('Test accuracy: ', test_acc)\n",
    "print('Test loss: ', test_loss)\n",
    "print('Test dcc: ', test_dcc)\n",
    "file_t = open(path_out + 'test_eval.txt','w')#append mode \n",
    "file_t.write(\"Test Accuracy: \" + str(test_acc)) \n",
    "file_t.write(\"\\n\")\n",
    "file_t.write(\"Test Loss: \" + str(test_loss)) \n",
    "file_t.write(\"\\n\")\n",
    "file_t.write(\"Test DCC: \" + str(test_dcc)) \n",
    "file_t.close() \n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "del c_01\n",
    "del c_02\n",
    "del c_03\n",
    "del c_04\n",
    "del d1\n",
    "del d2\n",
    "del d3\n",
    "del d4\n",
    "del d5\n",
    "del p1\n",
    "del p2\n",
    "del p3\n",
    "del p4\n",
    "del du_01\n",
    "del du_11\n",
    "del c_11\n",
    "del du_02\n",
    "del du_21\n",
    "del c_21\n",
    "del du_12\n",
    "del c_12\n",
    "del u_31\n",
    "del c_31\n",
    "del u_22\n",
    "del c_22\n",
    "del u_13\n",
    "del c_13\n",
    "del c_L\n",
    "gc.collect()\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "pred_path = path_out + 'Prediction/'\n",
    "\n",
    "if not os.path.exists(pred_path):\n",
    "    os.makedirs(pred_path)\n",
    "    print(\"Directory Created\")\n",
    "else:    \n",
    "    print(\"Directory Already Exists\") \n",
    "\n",
    "i = 0\n",
    "for idx in range(0,X_test.shape[0]):\n",
    "    i = i + 1\n",
    "    imageio.imwrite(pred_path + str(i) + '_Mask.jpg', Y_test[idx])\n",
    "    imageio.imwrite(pred_path + str(i) + '_PredMask.jpg', predicted_mask[idx])\n",
    "\n",
    "# Shows predicted outputs by choosing 10 Random images\n",
    "plt.figure(figsize=(8,30))\n",
    "i=1;total=10\n",
    "temp = np.ones_like( Y_test[0] )\n",
    "t = 0\n",
    "for idx in np.random.randint(0,high=X_test.shape[0],size=total):\n",
    "    t = t + 1\n",
    "    plt.subplot(total,3,i);i+=1\n",
    "    plt.imshow(X_test[idx], cmap='gray' )\n",
    "    plt.title(\"MRI\");plt.axis('off')\n",
    "    \n",
    "    plt.subplot(total,3,i);i+=1\n",
    "    plt.imshow(X_test[idx], cmap='gray' )\n",
    "    plt.imshow(temp - Y_test[idx], alpha=0.2, cmap='Set1' )\n",
    "    plt.title(\"Ground Truth\");plt.axis('off')\n",
    "    \n",
    "    plt.subplot(total,3,i);i+=1\n",
    "    plt.imshow(X_test[idx], cmap='gray' )\n",
    "    plt.imshow(temp - predicted_mask[idx],  alpha=0.2, cmap='Set1' )\n",
    "    plt.title(\"Predicted Tumor\");plt.axis('off')\n",
    "\n",
    "plt.savefig(path_out + 'Sample_Output_Plot.jpg')\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "\n",
    "#End Time\n",
    "print(\"--- Time : %s seconds ---\" % (time.time() - start_time))\n",
    "f_time = open(path_out + 'Time_Info.txt', 'w')\n",
    "print(\"%s %f\" % (\"Execution Time: \", time.time() - start_time), file=f_time)\n",
    "f_time.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
